# SPARK_AWS_PROJECT

### Sommaire


- [Description](#description)
- [Compétences](#how-to-use)
- [Technologies](#references)

---

## Description

Création d'un POC d'un environnement Big Data pour calcul distribué qui comprendra une première chaîne de traitement des données avec le preprocessing, et une étape de réduction de dimension avec Spark et le déploiement du modèle sur AWS.

## Compétences

- import des données; Source des données : https://www.kaggle.com/moltean/fruits  
- Création d'un compartiment sur le service S3 d'AWS
- Import des données sur le compartiment
- Création des clés de sécurité pour AWS
- Création d'une instance EC2 sur AWS et installation des modules nécessaires (Linux-SPARK-HADOOP-PYTHON...)
- Lancement du Notebook en cloud
- Connecter S3 avec EC2
- Import des données depuis S3 vers EC2
- Téléchargement du modèle avec "transfer learning"
- Réalisation d'une réduction de dimensionnalité avec Pyspark
- Import des résultats sur S3

## Technologies

- AWS
- SPARK
- PYSPARK
- S3
- Tensorflow
- EC2
- Python


[Back To The Top](#read-me-template)

