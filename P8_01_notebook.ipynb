{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630008c0",
   "metadata": {},
   "source": [
    "# Contexte:\n",
    "La réalisation d'un environnement Big Data: une première chaîne de traitement des données qui comprendra le preprocessing, feature extraction( basée sur transfer learning avec Resnet50) et une étape de réduction de dimension avec PCA.\n",
    "Cette réalisation sera effectuée à l'aide d'un script Pyspark et une instance cloud AWS EC2 pour l'exécution et une instance cloud AWS S3 pour sauvegarder les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2082ee6",
   "metadata": {},
   "source": [
    "On localise Spark dans l’EC2 avec findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00925683",
   "metadata": {},
   "source": [
    "On importe les modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# for connections to S3 AWS\n",
    "import boto3\n",
    "# pyspark modules\n",
    "import pyspark \n",
    "import urllib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, split\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "# tnesorflow modules\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb28435",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME']  ='/usr/lib/jvm/java-8-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd248c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars aws-java-sdk-bundle-1.12.178.jar, hadoop-aws-2.7.3.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f316cd1",
   "metadata": {},
   "source": [
    "On va réaliser laCommunication EC2-S3 avec boto3, et configurer la session Spark et le client S3. On va \n",
    "créer un contexte Spark capable de communiquer avec S3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb28723",
   "metadata": {},
   "source": [
    "\n",
    "## Vérification de la connexion EC2 <-> S3\n",
    "\n",
    "On vérifie la connexion et on liste le compartiment et les fichiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbaa31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "session = boto3.session.Session(aws_access_key_id='....',\n",
    "                                aws_secret_access_key='....')\n",
    "s3_client = session.client(service_name='s3', region_name='eu-west-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866797e",
   "metadata": {},
   "source": [
    "On vérifie la liste des compartiments ( 1 seul dans ce cas) et les fichiers qu'il contient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3433e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_client.list_buckets())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285a5c9",
   "metadata": {},
   "source": [
    "## Spark session kickoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder.master('local[*]')\n",
    "         .config('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "         #.config('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "         .appName('P8')\n",
    "         #.config('fs.s3a.aws.credentials.provider','com.amazonaws.auth.DefaultAWSCredentialsProviderChain')\n",
    "        #.config('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        #.config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3native.NativeS3FileSystem') \n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a891fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', '....')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', '....') \n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-west-1.amazonaws.com\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd39042",
   "metadata": {},
   "source": [
    "## Lecture des données de S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19da5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_url = \"s3a://sm-p8-fruits/fruits-360_dataset - Copie/fruits-360/*\"\n",
    "image_df = spark.read.format(\"binaryFile\").load(s3_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdaa15",
   "metadata": {},
   "source": [
    "Extraction du label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = images.withColumn('label', split(col('path'), '/').getItem(4))\n",
    "image_df = image_df.select('path', 'content', 'label')\n",
    "image_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d1fdc",
   "metadata": {},
   "source": [
    "Chargement des paramètres du Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False,\n",
    "                 input_shape=(100, 100, 3),\n",
    "                 pooling='max',\n",
    "                 weights=None)\n",
    "model.summary()  \n",
    "\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "model.set_weights(bc_model_weights.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "  \n",
    "    model = ResNet50(include_top=False,\n",
    "                     input_shape=(100, 100, 3),\n",
    "                     pooling='max',\n",
    "                     weights=None)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    img = Image.open(io.BytesIO(content)).resize([100, 100])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90790463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, url_series):\n",
    "    \n",
    "    input = np.stack(url_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "  model = model_fn()\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c12769",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sdf = image_sdf.select(col(\"path\"), col(\"label\"), featurize_udf(\"content\").alias(\"features\"))\n",
    "image_sdf.printSchema()\n",
    "image_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df = image_sdf.select(col(\"path\"),  col(\"label\"), list_to_vector_udf(image_sdf[\"features\"]).alias(\"features_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57995f",
   "metadata": {},
   "source": [
    "Normalisation des features avec standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features_',\n",
    "                              outputCol='feats_scaled')\n",
    "std = standardizer.fit(features_df)\n",
    "features_df_scaled = std.transform(features_df)\n",
    "features_df_scaled.show(5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7260b",
   "metadata": {},
   "source": [
    "Calcule de la réduction de dimensionnalité avec PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=50, inputCol=\"feats_scaled\", outputCol=\"pca\")\n",
    "modelpca = pca.fit(features_df_scaled)\n",
    "transformed = modelpca.transform(features_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2223306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = modelpca.explainedVariance.cumsum()\n",
    "sns.set_context(context='poster', font_scale=0.8)\n",
    "sns.lineplot(x=[i for i in range(51)], y=np.insert(var,0,0)*100, color='deepskyblue')\n",
    "plt.xlabel('PCs')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(left=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a266b",
   "metadata": {},
   "source": [
    "Exportation du résultats sur S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_final = transformed.select('image', 'label', 'pca').write.csv(\"s3a://sm-p8-fruits/pca_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aede33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
